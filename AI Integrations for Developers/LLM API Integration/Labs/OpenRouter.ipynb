{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2wnFb_4aBpP"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfdUb5dL4Fpq"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from pydantic import BaseModel\n",
        "\n",
        "def print_response(response):\n",
        "  print(f\"Response id: {response.id}\")\n",
        "\n",
        "  # NOTE: If `:free` models are used, the output tokens may be set to zero.\n",
        "  print(f\"Input tokens: {response.usage.input_tokens} ({response.usage.input_tokens_details.cached_tokens} cached); Output tokens: {response.usage.output_tokens} ({response.usage.output_tokens_details.reasoning_tokens} reasoning)\")\n",
        "  pprint(response.output)\n",
        "\n",
        "  print()\n",
        "  print(f\"{'-' * 20} [Text] {'-' * 20}\")\n",
        "  print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvF32EasJUEv"
      },
      "outputs": [],
      "source": [
        "prompts = {\n",
        "    \"business_naming\": \"Give me top 3 ideas for naming my business. It's a software company that innovates in audio recording.\",\n",
        "    \"describe_ai\": \"Describe AI as if it were an animal.\",\n",
        "    \"essay_about_humanity\": \"Write a short essay (up to 5 sentences) about the history of mankind.\",\n",
        "    \"generate_character\": \"Generate a fictional character of an adorable but nerdy teenager.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nplGB86mdeeq"
      },
      "source": [
        "## Basic usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5zSdzs8aYP7"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "client = OpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40nxP-YRPZvU"
      },
      "outputs": [],
      "source": [
        "available_models = client.models.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7pkJOhXPgES",
        "outputId": "fe35ce30-4c76-4bfe-9d3c-44f3c79ded3a"
      },
      "outputs": [],
      "source": [
        "pprint(available_models.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpOcGEoYbkPc"
      },
      "outputs": [],
      "source": [
        "simple_response = client.responses.create(\n",
        "    model=\"google/gemma-3-4b-it:free\",\n",
        "    input=prompts[\"essay_about_humanity\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cqFM0nPbr1i",
        "outputId": "5f376199-ed98-4f17-f87f-e34183739286"
      },
      "outputs": [],
      "source": [
        "print_response(simple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPoN3h5Q8zNS"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKIAeUIf81B6",
        "outputId": "5e0692d1-fb98-44f3-b9dc-0e3e306a9fa1"
      },
      "outputs": [],
      "source": [
        "with client.responses.stream(\n",
        "    model=\"google/gemma-3-4b-it:free\",\n",
        "    input=prompts[\"essay_about_humanity\"],\n",
        ") as stream:\n",
        "    for event in stream:\n",
        "        pprint(event)\n",
        "\n",
        "    streamed_response = stream.get_final_response()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j88tXs8Z9X9i",
        "outputId": "1d5fbf7f-2f2b-4ac7-bb23-ee399cc6e841"
      },
      "outputs": [],
      "source": [
        "print_response(streamed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY6ISED2dadC"
      },
      "source": [
        "## Reasoning\n",
        "\n",
        "For this section, pick models that support the `reasoning` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VX41dn1FAlE"
      },
      "outputs": [],
      "source": [
        "low_reasoning_response = client.responses.create(\n",
        "    model=\"minimax/minimax-m2.5:floor\",\n",
        "    input=prompts[\"business_naming\"],\n",
        "    reasoning={\"effort\": \"low\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGw2odpPJgAb",
        "outputId": "d0f61316-7914-462b-b8c8-ea49b743cddf"
      },
      "outputs": [],
      "source": [
        "print_response(low_reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7wTVJxZJc8S"
      },
      "outputs": [],
      "source": [
        "high_reasoning_response = client.responses.create(\n",
        "    model=\"minimax/minimax-m2.5:floor\",\n",
        "    input=prompts[\"business_naming\"],\n",
        "    reasoning={\"effort\": \"high\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhmatcDIJDLn",
        "outputId": "f2f8e04d-f440-4397-e5c5-b273baa2ff84"
      },
      "outputs": [],
      "source": [
        "print_response(high_reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTdQk8PXMgPy"
      },
      "source": [
        "## Temperature\n",
        "\n",
        "For this section, pick models that support the `temperature` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nm-gnHSLpAC"
      },
      "outputs": [],
      "source": [
        "deterministic_response = client.responses.create(\n",
        "    model=\"z-ai/glm-5:floor\",\n",
        "    input=prompts[\"describe_ai\"],\n",
        "    temperature=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLurcSSIM37W",
        "outputId": "e077ffa7-e0c1-4020-9182-e772f079da39"
      },
      "outputs": [],
      "source": [
        "print_response(deterministic_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONuZWhd7M9C9"
      },
      "outputs": [],
      "source": [
        "creative_response = client.responses.create(\n",
        "    model=\"z-ai/glm-5:floor\",\n",
        "    input=prompts[\"describe_ai\"],\n",
        "    temperature=0.9\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZTkA8bqNAg9",
        "outputId": "9feb87b2-e6bd-4a71-9e56-6c2a6bdfc60a"
      },
      "outputs": [],
      "source": [
        "print_response(creative_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhB_kDNP6YE"
      },
      "source": [
        "## Multi-turn conversations\n",
        "\n",
        "This example includes `instructions` giving the model high-level guidance on how it should behave while generating a response.\n",
        "\n",
        "> **OpenAI:**\n",
        "> _\"Note that the `instructions` parameter only applies to the current response generation request. If you are managing conversation state with the `previous_response_id` parameter, the `instructions` used on previous turns will not be present in the context.\"_\n",
        "\n",
        "Although the `previous_response_id` parameter exists, it is ignored.\n",
        "\n",
        "> **OpenRouter:**\n",
        "> _\"This API [the Responses API] is stateless - each request is independent and no conversation state is persisted between requests. You must include the full conversation history in each request.\"_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwxKCxKNPpCk"
      },
      "outputs": [],
      "source": [
        "multi_turn_conversation = [{ \"role\": \"user\", \"content\": \"Generate a short role-play in no more than 5 lines: manager and employee discussing performance issues.\" }]\n",
        "first_turn_response = client.responses.create(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    input=multi_turn_conversation,\n",
        "    instructions=\"The manager should talk like a pirate.\",\n",
        "    extra_body={\n",
        "        \"provider\": { \"sort\": { \"by\": \"throughput\" } }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgvu6oJEQrYr",
        "outputId": "2ffa26f9-f0cc-4a1a-d054-40e1edb51a80"
      },
      "outputs": [],
      "source": [
        "print_response(first_turn_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZIwZQoM9cMl",
        "outputId": "04011bd7-69e2-4bfd-ad9c-5864861efe0c"
      },
      "outputs": [],
      "source": [
        "multi_turn_conversation.extend(first_turn_response.output)\n",
        "pprint(multi_turn_conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2LGxE_aRBW8"
      },
      "outputs": [],
      "source": [
        "multi_turn_conversation.append({ \"role\": \"user\", \"content\": \"Keep the role-play going for another 5 lines, but introduce a new development: the employee's wife enters the room dramatically and tells him that she's pregnant, leaving him genuinely surpirsed but at the same time desperately hoping to save his job because his family depends on it.\" })\n",
        "second_turn_response = client.responses.create(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    input=multi_turn_conversation,\n",
        "    extra_body={\n",
        "        \"provider\": { \"sort\": { \"by\": \"throughput\" } }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWYWFfTWRWiH",
        "outputId": "8f1ef8e8-facb-4bc0-9d31-054a824f453d"
      },
      "outputs": [],
      "source": [
        "print_response(second_turn_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjlH03a9U33r"
      },
      "source": [
        "## Structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKDYJPPscw5l"
      },
      "outputs": [],
      "source": [
        "no_structure_response = client.responses.create(\n",
        "    model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n",
        "    input=prompts[\"generate_character\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRW7Nfspc3m9",
        "outputId": "cac194d8-666d-4923-fa5b-f1dcd9813a5d"
      },
      "outputs": [],
      "source": [
        "print_response(no_structure_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28GHDOl5aH9U"
      },
      "source": [
        "`Structured output` evolves from `JSON mode`, however it is no longer recommended to use the latter for a number of reasons.\n",
        "\n",
        "> **OpenAI:**\n",
        "> _When using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit._\n",
        "\n",
        "> **OpenAI:**\n",
        "> _JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1fkpWC8U4NF"
      },
      "outputs": [],
      "source": [
        "diy_structured_response = client.responses.create(\n",
        "    model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n",
        "    input=prompts[\"generate_character\"],\n",
        "    text={\n",
        "        \"format\": {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"name\": \"character\",\n",
        "            \"schema\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                  \"name\": {\"type\": \"string\", \"description\": \"The character's full name\"},\n",
        "                  \"age\": {\"type\": \"integer\", \"description\": \"The character's age\"},\n",
        "                  \"hobby\": {\"type\": \"string\", \"description\": \"The character's hobby\"}\n",
        "              },\n",
        "              \"required\": [\"name\", \"age\", \"hobby\"],\n",
        "              \"additionalProperties\": False,\n",
        "            },\n",
        "            \"strict\": True\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQrSZFghVRV_",
        "outputId": "7bd651b7-5087-47e8-e1a3-5f65613c0363"
      },
      "outputs": [],
      "source": [
        "print_response(diy_structured_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdSwHS70a2E1"
      },
      "outputs": [],
      "source": [
        "class Character(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    hobby: str\n",
        "\n",
        "auto_structured_response = client.responses.parse(\n",
        "    model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n",
        "    input=prompts[\"generate_character\"],\n",
        "    text_format=Character\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYM0x0vjcWM_",
        "outputId": "08a10a13-3671-4aae-c0dd-d3327ad25d1b"
      },
      "outputs": [],
      "source": [
        "print_response(auto_structured_response)\n",
        "\n",
        "print()\n",
        "print(f\"Parsed output: {auto_structured_response.output_parsed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOcS5WSg3y1_"
      },
      "source": [
        "## Vision\n",
        "\n",
        "<img src=\"https://freerangestock.com/sample/88947/painter-working-in-studio.jpg\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD0t-xfw30HP"
      },
      "outputs": [],
      "source": [
        "image_analysis_response = client.responses.create(\n",
        "    model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert image analyst. Keep your answer concise and structured.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                { \"type\": \"input_text\", \"text\": \"Analyze this image and return a one-sentence summary followed by 5 key visible objects.\" },\n",
        "                { \"type\": \"input_image\", \"image_url\": \"https://freerangestock.com/sample/88947/painter-working-in-studio.jpg\" }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktGfo8Rq7bnP",
        "outputId": "9b74d098-11ef-4fbc-c4b5-788e73eb3ef7"
      },
      "outputs": [],
      "source": [
        "print_response(image_analysis_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WjlH03a9U33r",
        "VOcS5WSg3y1_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
